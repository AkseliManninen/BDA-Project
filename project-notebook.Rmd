---
title: "BDA Project"
author: "Niko Miller, Akseli Manninen and Santeri Löppönen"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: '3'
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(aaltobda)
library(tidyverse)
library(latex2exp)
library(loo)
library(bayesplot)
library(posterior)
library(rstan)
library(tidyverse)
library(reshape2)
library(ggcorrplot)
```

\newpage
# 1. Project Description and Motivation

Studying the relationship between income and education has been the focus on many studies. The studies have 
concluded that there is a strong correlation between higher education and income (Card, 1999). In general, 
individuals with stronger education are more likely to be employed and earn a big salary compared to less 
educated people (Card, 1999). For that reason, education is described as an investment in human capital 
(Wolla & Sullivan, 2017). 

This study examines this phenomenon from the perspective of people that have acquired their education
from colleges of the United States. As the connection between education and income has been
shown in the existing literature, this study strives to examine the associations between college related
features and income level years after graduation. This project is not limited to only 
considering educational aspects but expands it to family backgrounds.

In this study, a Bayesian approach is taken to observe the bond between the educational and family related 
features and earnings. It is in our interest to find out, how accurately the selected features can predict future 
income for the students. Furthermore, finding a well-predictive features among the vast number of
variables is pursed and evaluating the predictive performance of selected statistical models. As this study is conducted 
in a university environment by university students, the possible insight would be especially meaningful for the 
members of the group and peers.

# 2. Data and the analysis problem

We use the most recent institutional-level college scorecard data from the US Department of Education. 
The institutional-level dataset contains aggregate data for each educational institution and includes data on 
institutional characteristics, enrollment, student aid, costs and student outcomes. 
The dataset has over 6000 observations on more than 3000 variables.

We chose to use this dataset because we could use the data to answer interesting education-related questions in our project, 
and also because the dataset has a large number of observations and also a large number of variables. 
The large amount of variables means we would have a lot of flexibility when it came to modelling our data and there would be 
enough data to possibly make valid inferences due to many observations. Of course, the large amount of variables also 
posed challenges, as it was arguably slower and more burdensome to find the most relevant variables to use in our analysis. 
We were somewhat surprised by how fast the observation count started to shrink when we began cleaning the data, 
so in hindsight we should have maybe paid more attention to cleanliness, as this dataset had for example a lot of missing values

```{r}
# read in data sets
data <- read.csv2("./Data/Most-Recent-Cohorts-Institution.csv", sep = ",", fileEncoding="UTF-8-BOM") %>% as_tibble()
data.description <- read.csv2("./Data/CollegeScorecardDataDictionary.csv", sep = ",", fileEncoding="UTF-8-BOM") %>% as_tibble()
```

```{r}
dim(data)
sum(data == "NULL")
```

## Feature selection

The initial data set had almost 3000 features and in that regards, the number
of observations is relatively small. There are also a lot of missing values
in the data set and some features are missing. For these reasons, there was
a need to prune features.

The used process of feature selection consisted of two phases: In the first phase,
a subset of features was select based on the features used in the existing 
literature and using domain knowledge. From the potential features, only those
having enough data were included in the subset and others were discarded.

The selected features were: 

NOTE: The last variable MD_EARN_WNE_P10 is the dependent variable.

$$
\begin{array} {c|c|c|c|c|c|c|c} & name & data\:type & description \\ 
\hline
1 & SATVRMID & int & Midpoint\:of\:SAT\:scores\:at\:the\:institution\:(critical\:reading) \\
2 & SATMTMID & int & Midpoint\:of SAT scores at the institution (math) \\
3 & SATWRMID & int & Midpoint\:of\:SAT\:scores\:at\:the\:institution\:(writing) \\
4 & MD\_FAMINC & float [0,1] & Median\:family\:income \\
5 & AGE\_ENTRY & float & Average\:age\:of\:entry \\
6 & FEMALE & float [0,1] & Share\:of\:female\:students \\
7 & FIRST\_GEN & float [0,1] & Share\:of\:first-generation students \\
8 & PCT\_WHITE & float [0,1] & Percent\:of\:the\:population\:from\:students'\:zip\:codes\:that\:is\:White \\
9 & DEBT\_MDN\_SUPP & float & Median\:debt,\:suppressed\:for\:n=30 \\
10 & C150\_4 & float[0,1] & Completion\:rate\:for\:first-time,\:full-tim\:students \\
11 & COSTT4\_A & float[0,1] & Average\:cost\:of\:attendance\:(academic\:year\:institutions) \\
12 & POVERTY\_RATE & float[0,1] & Poverty\:rate \\
13 & UNEMP\_RATE & float[0,1] & Unemployment\:rate \\
14 & MARRIED & float[0,1] & Share\:of\:married students \\
15 & VETERAN & float[0,1] & Share\:of\:veteran students \\
16 & LOCALE & categorical & Locale\:of\:institution \\
17 & CCBASIC & categoriacal & Carnegie\:Classification -- basic \\
18 & CONTROL & categorical & Control\:of\:institution \\
19 & MD\_EARN\_WNE\_P10 & float & Median\:earnings\:of\:students\:working\:and\:not\:enrolled\:10\:years\:after\:entry\\
\end{array}
$$

## Code for extracting the subset features
```{r, warning = FALSE}

# DATA MANIPULATION ----

# list variables
id.vars <- c("UNITID", "INSTNM", "CITY", "ST_FIPS", "REGION")
numerical.vars <- c("SATVRMID", "SATMTMID", "SATWRMID", "MD_FAMINC", "AGE_ENTRY", "FEMALE", "FIRST_GEN", "PCT_WHITE", "DEBT_MDN_SUPP", "C150_4", "COSTT4_A", "MD_EARN_WNE_P10", "POVERTY_RATE", "UNEMP_RATE", "MARRIED", "VETERAN")
categorical.vars <- c("LOCALE", "CCBASIC", "CONTROL")
SAT.vars <- c("SATVRMID", "SATMTMID", "SATWRMID") # helper variable later on

# create map for variable descriptions
variable.descriptions <- data.description %>%
  select(VARIABLE.NAME, NAME.OF.DATA.ELEMENT, NOTES) %>%
  filter(VARIABLE.NAME %in% c(id.vars, categorical.vars, numerical.vars))

# extract categorical variables
data.filtered.categorical <- data %>%
  select(all_of(id.vars), all_of(categorical.vars)) %>%
  mutate(across(.cols = all_of(categorical.vars), .fns = as.factor))

# drop rows with NA for categorical vars
data.filtered.categorical.dropna <- data.filtered.categorical %>%
  drop_na()

# extract numerical variables
data.filtered.numerical <- data %>%
  select(all_of(id.vars), all_of(numerical.vars)) %>%
  mutate(across(.cols = c(id.vars[1], numerical.vars), .fns = as.numeric))

# drop rows with NA for numerical vars
data.filtered.numerical.dropna <- data.filtered.numerical %>%
  drop_na()

# join categorical and numerical variables by id
data.filtered.numerical.dropna <- data.filtered.numerical.dropna %>%
  mutate(SAT_ALL = data.filtered.numerical.dropna %>%
           select(SATVRMID, SATMTMID, SATWRMID) %>%
           rowMeans())

data.filtered.all <- data.filtered.numerical.dropna %>%
  inner_join(data.filtered.categorical.dropna, by = id.vars)
```

In the second phase of feature selection, a subset of features was selected
from the 18 variables of the first phase. The correlations between the features 
were examined as well as their associations to the dependent variable.

## Numerical variables

SAT scores were combined as one variable, because they were correlated and
viewed as one entity. However, writing SAT scores had too few observations,
due to discontinued tracking, so the variable SAT_ALL was formed by calculating
the average of math and critical thinking SAT scores. SAT scores were included
because the correlation was high with the dependent variable.

For the rest of the numerical variables, if the correlation between a feature
and the dependent variable was low and there was not observable dependency 
between the two in the scatter plot, the feature was excluded. Also, if multiple
independent variables were highly correlated, for the sake of simplicity only
one of them was selected to the model, based on the highest correlation and 
dependency with the dependent variable.

The selected numerical variables were: SAT_ALL, MD_FAMINIC, AGE_ENTRY, 
COSTT4_A, and POVERTY_RATE.


## Categorical variables

The categorical variables were observed with box plots to see if there were 
differences between the categories and the dependent variable. Based on an 
analysis on the variable LOCALE, a new variable was CITY was generated
which represents, whether ... 


The selected categorical variables were: LOCALE, CCBASIC and CONTROL.



```{r, warning = FALSE}
# PRELIMINARY ANALYSIS ----

# summarize data
summary(data.filtered.all)

# make variable type specific data frames for plots etc.
numerical.vars.data <- data.filtered.all %>% select(!c(id.vars, categorical.vars, SAT.vars)) %>% relocate(MD_EARN_WNE_P10, 1)
categorical.vars.data <- data.filtered.all %>% select(all_of(categorical.vars))

# visualizations
corrplot::corrplot(cor(numerical.vars.data))
melted <- melt(numerical.vars.data, id.vars = "MD_EARN_WNE_P10")

ggplot(melted, aes(x = value, y = MD_EARN_WNE_P10)) +
  facet_wrap(~variable, scales = "free") +
  geom_point()

# preliminary model with all numerical vars (not yet categorical)
model <- lm(MD_EARN_WNE_P10 ~ ., data = numerical.vars.data)
summary(model)

# step wise regression implied "best" model in terms of AIC
step(model)
stepwise.model <- lm(MD_EARN_WNE_P10 ~ FEMALE + C150_4 + COSTT4_A + POVERTY_RATE +
                       UNEMP_RATE + MARRIED + SAT_ALL, data = numerical.vars.data)
summary(stepwise.model)

# numerical + categorical vars model
full.model.data <- data.filtered.all %>%
  select(!c(id.vars, SAT.vars))
model.full <- lm(MD_EARN_WNE_P10 ~ ., full.model.data)
summary(model.full)

```

## Variable correlation matrix
```{r, warning = FALSE, fig.align='left', out.width='70%', optipng = '-o7'}

# comment

r <- cor(numerical.vars.data)
ggcorrplot(r,
           hc.order = TRUE,
           lab = TRUE,
           lab_size = 3.5)

```
Finally, the variables were tested with...

The results were intuitive, suggested including all of the variables --> 
Stays the same

Secondly, step wise regression was used to find the features resulting 
with the best AIC score.


# 3. Description of the models

Description of at least two models, for example:
- non hierarchical and hierarchical,
- linear and non linear,
- variable selection with many models.

## Description of the separate model

Add LaTeX!

## Description of the pooled model

Add LaTeX!

## Description of the hierarchical model

Add LaTeX!

---Linear model??? Should we describe this

# 4. Informative or weakly informative priors, and justification of their choices.

# 5. Stan, rstanarm or brms code.

# 6. How to the Stan model was run, that is, what options were used. 
This is also more clear as combination of textual explanation and the actual code line.

# 7. Convergence diagnostics (Rˆ, ESS, divergences) and what was done if the convergence was not good with the first try. 
This should be reported for all models.

# 8. Posterior predictive checks and what was done to improve the model. This should be reported for all models.

# 9. Optional/Bonus: Predictive performance assessment if applicable (e.g. classification accuracy) and evaluation 
of practical usefulness of the accuracy. This should be reported for all models as well.

# 10. Sensitivity analysis with respect to prior choices (i.e. checking whether the result changes a lot if prior 
is changed). This should be reported for all models.

# 11. Discussion of issues and potential improvements.

# 12. Conclusion what was learned from the data analysis.

# 13. Self-reflection of what the group learned while making the project.
 
# 14. References

Insert bibliography here
Card, D. (1999). THE CAUSAL EFFECT OF EDUCATION ON EARNINGS.
Wolla, S. A., & Sullivan, J. (2017). Education, Income, and Wealth. https://fred.stlouisfed.org/graph/?g=7yKu.
