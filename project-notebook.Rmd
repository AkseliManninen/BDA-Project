---
title: "BDA Project"
author: "Niko Miller, Akseli Manninen and Santeri Löppönen"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: '3'
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(aaltobda)
library(tidyverse)
library(latex2exp)
library(loo)
library(bayesplot)
library(posterior)
library(rstan)
library(tidyverse)
library(reshape2)
library(ggcorrplot)
```

\newpage
# 1. Project Description and Motivation

Studying the relationship between income and education has been the focus on many studies. The studies have 
concluded that there is a strong correlation between higher education and income (Card, 1999). In general, 
individuals with stronger education are more likely to be employed and earn a big salary compared to less 
educated people (Card, 1999). For that reason, education is described as an investment in human capital 
(Wolla & Sullivan, 2017). 

This study examines this phenomenon from the perspective of people that have acquired their education
from colleges of the United States. As the connection between education and income has been
shown in the existing literature, this study strives to examine the associations between college related
features and income level years after graduation. This project is not limited to only 
considering educational aspects but expands it to family backgrounds.

In this study, a Bayesian approach is taken to observe the bond between the educational and family related 
features and earnings. It is in our interest to find out, how accurately the selected features can predict future 
income for the students. Furthermore, finding a well-predictive features among the vast number of
variables is pursed and evaluating the predictive performance of selected statistical models. As this study is conducted 
in a university environment by university students, the possible insight would be especially meaningful for the 
members of the group and peers.

# 2. Data and the analysis problem

We use the most recent institutional-level college scorecard data from the US Department of Education. 
The institutional-level dataset contains aggregate data for each educational institution and includes data on 
institutional characteristics, enrollment, student aid, costs and student outcomes. 
The dataset has over 6000 observations on more than 3000 variables.

We chose to use this dataset because we could use the data to answer interesting education-related questions in our project, 
and also because the dataset has a large number of observations and also a large number of variables. 
The large amount of variables means we would have a lot of flexibility when it came to modelling our data and there would be 
enough data to possibly make valid inferences due to many observations. Of course, the large amount of variables also 
posed challenges, as it was arguably slower and more burdensome to find the most relevant variables to use in our analysis. 
We were somewhat surprised by how fast the observation count started to shrink when we began cleaning the data, 
so in hindsight we should have maybe paid more attention to cleanliness, as this dataset had for example a lot of missing values

```{r}
# read in data sets
data <- read.csv2("./Data/Most-Recent-Cohorts-Institution.csv", sep = ",", fileEncoding="UTF-8-BOM") %>% as_tibble()
data.description <- read.csv2("./Data/CollegeScorecardDataDictionary.csv", sep = ",", fileEncoding="UTF-8-BOM") %>% as_tibble()
```

```{r}
dim(data)
sum(data == "NULL")
```

## Feature selection

## Phase 1 - Feature selection based on literature and domain knowledge

The initial data set had almost 3000 features and in that regards, the number
of observations is relatively small. There are also a lot of missing values
in the data set and some features are missing. For these reasons, there was
a need to prune features.

The used process of feature selection consisted of two phases: In the first phase,
a subset of features was select based on the features used in the existing 
literature and using domain knowledge. From the potential features, only those
having enough data were included in the subset and others were discarded.

The selected features were: 

$$
\begin{array} {c|c|c|c|c|c|c|c} & name & data\:type & description \\ 
\hline
1 & SATVRMID & int & Midpoint\:of\:SAT\:scores\:at\:the\:institution\:(critical\:reading) \\
2 & SATMTMID & int & Midpoint\:of SAT scores at the institution (math) \\
3 & SATWRMID & int & Midpoint\:of\:SAT\:scores\:at\:the\:institution\:(writing) \\
4 & MD\_FAMINC & float & Median\:family\:income \\
5 & AGE\_ENTRY & float & Average\:age\:of\:entry \\
6 & FEMALE & float [0,100] & Share\:of\:female\:students \\
7 & FIRST\_GEN & float [0,100] & Share\:of\:first-generation students \\
8 & PCT\_WHITE & float [0,100] & Percent\:of\:the\:population\:from\:students'\:zip\:codes\:that\:is\:White \\
9 & DEBT\_MDN\_SUPP & float & Median\:debt,\:suppressed\:for\:n=30 \\
10 & C150\_4 & float[0,100] & Completion\:rate\:for\:first-time,\:full-tim\:students \\
11 & COSTT4\_A & float & Average\:cost\:of\:attendance\:(academic\:year\:institutions) \\
12 & POVERTY\_RATE & float[0,100] & Poverty\:rate \\
13 & UNEMP\_RATE & float[0,100] & Unemployment\:rate \\
14 & MARRIED & float[0,100] & Share\:of\:married students \\
15 & VETERAN & float[0,100] & Share\:of\:veteran students \\
16 & LOCALE & categorical & Locale\:of\:institution \\
17 & CCBASIC & categoriacal & Carnegie\:Classification -- basic \\
18 & CONTROL & categorical & Control\:of\:institution \\
19 & MD\_EARN\_WNE\_P10 & float & Median\:earnings\:of\:students\:working\:and\:not\:enrolled\:10\:years\:after\:entry \:(dependent\:variable)\\
\end{array}
$$

## Code for extracting the subset features
```{r, warning = FALSE}

# DATA MANIPULATION ----

# list variables
id.vars <- c("UNITID", "INSTNM", "CITY", "ST_FIPS", "REGION")
numerical.vars <- c("SATVRMID", "SATMTMID", "SATWRMID", "MD_FAMINC", "AGE_ENTRY", "FEMALE", "FIRST_GEN", "PCT_WHITE", "DEBT_MDN_SUPP", "C150_4", "COSTT4_A", "MD_EARN_WNE_P10", "POVERTY_RATE", "UNEMP_RATE", "MARRIED", "VETERAN")
categorical.vars <- c("LOCALE", "CCBASIC", "CONTROL")
SAT.vars <- c("SATVRMID", "SATMTMID", "SATWRMID") # helper variable later on

# create map for variable descriptions
variable.descriptions <- data.description %>%
  select(VARIABLE.NAME, NAME.OF.DATA.ELEMENT, NOTES) %>%
  filter(VARIABLE.NAME %in% c(id.vars, categorical.vars, numerical.vars))

# extract categorical variables
data.filtered.categorical <- data %>%
  select(all_of(id.vars), all_of(categorical.vars)) %>%
  mutate(across(.cols = all_of(categorical.vars), .fns = as.factor))

# drop rows with NA for categorical vars
data.filtered.categorical.dropna <- data.filtered.categorical %>%
  drop_na()

# extract numerical variables
data.filtered.numerical <- data %>%
  select(all_of(id.vars), all_of(numerical.vars)) %>%
  mutate(across(.cols = c(id.vars[1], numerical.vars), .fns = as.numeric))

# drop rows with NA for numerical vars
data.filtered.numerical.dropna <- data.filtered.numerical %>%
  drop_na()

# join categorical and numerical variables by id
data.filtered.numerical.dropna <- data.filtered.numerical.dropna %>%
  mutate(SAT_ALL = data.filtered.numerical.dropna %>%
           select(SATVRMID, SATMTMID, SATWRMID) %>%
           rowMeans())

data.filtered.all <- data.filtered.numerical.dropna %>%
  inner_join(data.filtered.categorical.dropna, by = id.vars)
```

## Phase 2 - Feature selection wiht correlation and visual dependency


In the second phase of feature selection, a subset of features was selected
from the 18 variables of the first phase. The correlations between the features 
were examined as well as their associations to the dependent variable.

## Numerical variables

SAT scores were combined as one variable, because they were correlated and
viewed as one entity. However, writing SAT scores had too few observations,
due to discontinued tracking, so the variable SAT_ALL was formed by calculating
the average of math and critical thinking SAT scores. SAT scores were included
because the correlation was high with the dependent variable.

For the rest of the numerical variables, if the correlation between a feature
and the dependent variable was low and there was not observable dependency 
between the two in the scatter plot, the feature was excluded. Also, if multiple
independent variables were highly correlated, for the sake of simplicity only
one of them was selected to the model, based on the highest correlation and 
dependency with the dependent variable.

The selected numerical variables were: SAT_ALL, MD_FAMINIC, AGE_ENTRY, 
COSTT4_A, and POVERTY_RATE.


## Categorical variables

The categorical variables were observed with box plots to see if there were 
differences between the categories and the dependent variable. Based on an 
analysis on the variable LOCALE, a new binary variable URBAN was generated
which represents, where value 1 represents city-like area which includes
categories Large City, Mid-Size City, Urban Fringe of a Large City and Urban 
Fringe of a Mid-Size City of the LOCALE variable from the dataset.

The categorical variable CCBASIC which represented the Carnegie Classification
was divided into two binary MASTER and DOCTORL. These variables represent
if the college is classified as Master's college and university or Doctoral's
college and university. If a college does not belong to either of those,
it is a Bachelor's college and university. Other special focus colleges
and universities were discarded from the dataset to keep the model more simple.

The categorical variable CONTROL, which had three classes Public, Private
and Private, Nonprofit and Proprietary was modified into binary variable
Private representing if the school is private or public school. There
were only few Proprietary observations, so those were discarded.

The selected categorical variables were: URBAN, DOCTORAL, MASTER, PRIVATE.

```{r, warning = FALSE}
# PRELIMINARY ANALYSIS ----

# summarize data
summary(data.filtered.all)

# make variable type specific data frames for plots etc.
numerical.vars.data <- data.filtered.all %>% select(!c(id.vars, categorical.vars, SAT.vars)) %>% relocate(MD_EARN_WNE_P10, 1)
categorical.vars.data <- data.filtered.all %>% select(all_of(categorical.vars))

# visualizations
corrplot::corrplot(cor(numerical.vars.data))
melted <- melt(numerical.vars.data, id.vars = "MD_EARN_WNE_P10")

ggplot(melted, aes(x = value, y = MD_EARN_WNE_P10)) +
  facet_wrap(~variable, scales = "free") +
  geom_point()

# preliminary model with all numerical vars (not yet categorical)
model <- lm(MD_EARN_WNE_P10 ~ ., data = numerical.vars.data)
summary(model)

# step wise regression implied "best" model in terms of AIC
step(model)
stepwise.model <- lm(MD_EARN_WNE_P10 ~ FEMALE + C150_4 + COSTT4_A + POVERTY_RATE +
                       UNEMP_RATE + MARRIED + SAT_ALL, data = numerical.vars.data)
summary(stepwise.model)

# numerical + categorical vars model
full.model.data <- data.filtered.all %>%
  select(!c(id.vars, SAT.vars))
model.full <- lm(MD_EARN_WNE_P10 ~ ., full.model.data)
summary(model.full)

```

## Variable correlation matrix
```{r, warning = FALSE, fig.align='left', out.width='70%', optipng = '-o7'}

# comment

r <- cor(numerical.vars.data)
ggcorrplot(r,
           hc.order = TRUE,
           lab = TRUE,
           lab_size = 3.5)

```

## Phase 3 - Feature selection with Stepwise regression

In the third phase, the remaining variables were used with Stepwise
regression, to test which subset of features perform the best, and
whether the received coefficients are reasonable, and if there are signs
of overfitting.

The Stepwise regression suggested using all of the variables, except CITY
and DOCTORAL.

The final features are listed in the table below:

$$
\begin{array} {c|c|c|c|c|c|c|c} & name & data\:type & description \\ 
\hline
1 & SAT\_ALL & int & Midpoint\:of\:SAT\:scores\:at\:the\:institution\:(Average\:of\:critical\:reading\:math) \\
2 & MD\_FAMINC & float & Median\:family\:income \\
3 & AGE\_ENTRY & float & Average\:age\:of\:entry \\
4 & COSTT4\_A & float & Average\:cost\:of\:attendance\:(academic\:year\:institutions) \\
5 & POVERTY\_RATE & float[0,100] & Poverty\:rate \\
6 & PRIVATE & binary & Carnegie\:Classification -- basic \\
7 & MASTER & binary & Control\:of\:institution \\
8 & MD\_EARN\_WNE\_P10 & float & Median\:earnings\:of\:students\:working\:and\:not\:enrolled\:10\:years\:after\:entry \:(dependent\:variable)\\
\end{array}
$$


# 3. Description of the models

Description of at least two models, for example:
- non hierarchical and hierarchical,
- linear and non linear,
- variable selection with many models.

## Description of the separate model

Add LaTeX!

## Description of the pooled model

Add LaTeX!

## Description of the hierarchical model

Add LaTeX!

---Linear model??? Should we describe this

# 4. Informative or weakly informative priors, and justification of their choices.

SAT_ALL  Norm(mu, 500)
-	mu = Keskimääräinen tulo / keskimääräinen SAT tulos (lähteet), iso hajonta, ei rajoteta nollaan.
-	SAT jossain määrin positiivisesti priorin perusteella, mutta ison hajonnan perusteella voi olla negatiivinen. Iso hajonta, koska palkka selittyy monista tekijöistä.
MD_FAMINIC (median family income)  Norm(0, 100)
-	Ei tiedetä miten vaikuttaa
-	Median income absoluuttiset luvut ovat isoja (isompia, kuin vaikka SAT)  Pienempi hajonta kuin SAT, koska etsitään painoarvoa.
-	Mutta vanhemmat voivat olla köyhiä, mutta lapsi tienaa paljon rahaa.
AGE_ENTRY  Norm(0, 2500)
-	Relatistisesti 10 – 50v, suurin osa on 20 v.
-	Iso hajonta. Esimerkiksi 50v aloitus, palkka voi olla 50k pienempi, jolloin beta olisi – 1000.
COSTT4_A (Average cost of attendance)  MD_FAMINIC (median family income)  Norm(0, 500)
-	Pienempi arvoja kuin family income. 
-	Suuritulosesta perheestä ei varaa lähettää montaa lasta yliopistoo, vs. ainut lapsi. Nämä kohdistuu suoraan lapseen vs. perheen tulot, ei välttämättä indikoi, paljon lapsi saa rahaa. 
POVERTY_RATE  Norm(0, 2500)
-	Arvot pieniä  Isompi hajonta.
-	Tarvitsee tosi ison painon, jos työttömyyttä on vähän. Absoluuttinen arvo voilla olla tosi pieni, esim 0.01  Tarvitsee ison hajonna.
MASTER  Normal(0, 25 000)
-	Oletetaan ettei vaikuta, mutta on mahdollista, että vaikuttaa palkkaan suurestikin. Estimaatin mahdolliset arvot oltava isoja, koska dummy muuttujan on oltava aina 0 tai 1.
PRIVATE  Normal(0, 25 000)
-	Oletetaan ettei vaikuta, mutta on mahdollista, että vaikuttaa palkkaan suurestikin. Estimaatin mahdolliset arvot oltava isoja, koska dummy muuttujan on oltava aina 0 tai 1.




# 5. Stan, rstanarm or brms code.

# 6. How to the Stan model was run, that is, what options were used. 
This is also more clear as combination of textual explanation and the actual code line.

# 7. Convergence diagnostics (Rˆ, ESS, divergences) and what was done if the convergence was not good with the first try. 
This should be reported for all models.

# 8. Posterior predictive checks and what was done to improve the model. This should be reported for all models.

# 9. Optional/Bonus: Predictive performance assessment if applicable (e.g. classification accuracy) and evaluation 
of practical usefulness of the accuracy. This should be reported for all models as well.

# 10. Sensitivity analysis with respect to prior choices (i.e. checking whether the result changes a lot if prior 
is changed). This should be reported for all models.

# 11. Discussion of issues and potential improvements.

- Correlation does not mean causality
- Ethics selecting features
- 

# 12. Conclusion what was learned from the data analysis.

# 13. Self-reflection of what the group learned while making the project.
 
# 14. References

Insert bibliography here
Card, D. (1999). THE CAUSAL EFFECT OF EDUCATION ON EARNINGS.
Wolla, S. A., & Sullivan, J. (2017). Education, Income, and Wealth. https://fred.stlouisfed.org/graph/?g=7yKu.
